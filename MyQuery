Adding Hive Queries used with different Data sets

https://github.com/hortonworks/hadoop-tutorials/tree/master/Sandbox
http://cdn.oreillystatic.com/oreilly/booksamplers/packt/9781783285471_Sample.pdf
https://github.com/hortonworks/hive-testbench/tree/hive14/settings  -- Hive settings used in a typical hortonwork hadoop
https://github.com/apache/hive/tree/master/data/files
https://github.com/ananddhanabal/hive/blob/master/HIVE%20Tutorial   -- good text on hive with examples
https://github.com/Alekyab/Hive
https://github.com/Alekyab/oozie
https://github.com/yhemanth/hive-samples
https://github.com/Re1tReddy/Hive-Pig-Hbase

 Lots of links to explore for hadoop AWESOME
https://github.com/rameshsurapathi/awesome-hadoop#sql-on-hadoop
https://github.com/rameshsurapathi/Tech_Notes

--------------------------------------------Book Data--------Hive/Pig Start--------------------------------------------

Download file -> http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip

Run the following unix commands to clean the data.
• sed 's/\&amp;/\&/g' BX-Books.csv | sed -e '1d' |sed 's/;/$$$/g' | sed 's/"$$$"/";"/g'|sed 's/\"//g' > BX-BooksC.csv
• sed 's/\&amp;/\&/g' BX-Users.csv | sed -e '1d' |sed 's/\"//g' > BX-UsersC.csv
• sed 's/\&amp;/\&/g' BX-Book-Ratings.csv | sed -e '1d' |sed 's/;/$$$/g' | sed 's/"$$$"/";"/g'|sed 's/\"//g' > BX-BookRatingC.csv

Create a folder "Data" in hdfs and Move all the 3 cleaned csv files to hdfs /user/cloudera/Data and run chmod 777 for all the files
If you want the files from the local filesystem then change the LOAD statement to include INPATH LOCAL

• HDFS User Permissions - login into hortonworks 2.3.1 root/hadoop
	• su -- hdfs -c "hadoop fs -chmod -R 755 /user"
	• su -- hdfs -c "hadoop fs -chmod 755 /user/hive"
	• hive \ -hiveconf DATA_DIR=…/hcb-v2/chapter6/data/ \ -f create-book-crossing.hql 
	• hive -e 'select * from bookcrossing.users limit 10'  -----from unix shell 
	• Wget http://hortonassets.s3.amazonaws.com/tutorial/hive/Twitterdata.txt
	• Hadoop fs -put Twitterdata.txt /user/hive/Data
	• Hadoop fs -ls /user/hive/Data
	• Make sure the owner of the file is admin, upload the files from ambari file browser or change owner for the files.

#http://mapredit.blogspot.com/2013/05/get-all-extended-hive-tables-with.html
Get all extended Hive tables with location in HDFS
for file in $(hive -e "show table extended like \`*\`" | grep location: | awk 'BEGIN { FS = ":" };{printf("hdfs:%s:%s\n",$3,$4)}'); do hdfs dfs -du -h $file; done;

 hive log to console
 -hiveconf hive.root.logger=DEBUG,console

CREATE DATABASE IF NOT EXISTS bookcrossing;

USE bookcrossing;

SET hive.cli.print.header=true;

Note : All the HDFS files from /user/hive/Data are moved to hive metastore warehouse directory /apps/hive/warehouse after table is create with data in HDP 2.3.1

Books data ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS BookData 
 (ISBN STRING, 
 BookTitle STRING, 
 BookAuthor STRING, 
 YearOfPublication INT, 
 Publisher STRING)
 ROW FORMAT DELIMITED 
 FIELDS TERMINATED BY '\59'
 STORED AS TEXTFILE;

LOAD DATA INPATH '/user/hive/Data/BX-BooksC.csv'
 OVERWRITE INTO TABLE BookData;

CREATE EXTERNAL TABLE IF NOT EXISTS books 
(isbn INT, title STRING, author STRING, year INT, publisher STRING, image_s STRING, image_m STRING,  image_l STRING) COMMENT 'Book crossing books list cleaned' 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\073' 
STORED AS TEXTFILE
LOAD DATA INPATH '/user/hive/Data/BX-Books-Prepro.txt'
 OVERWRITE INTO TABLE Books;


Books Rating data ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS BookRating 
  (UserId STRING, 
  ISBN STRING, 
  Rating INT)
  ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '\59'
  STORED AS TEXTFILE;
  
 LOAD DATA INPATH '/user/hive/Data/BX-BookRatingC.csv'
 OVERWRITE INTO TABLE BookRating;

CREATE TABLE book_ratingi (USER_ID INT, ISBN STRING, Rating INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";
INSERT OVERWRITE TABLE book_ratingi
select regexp_replace(UserId,"\"",""), regexp_replace(ISBN,">",""), regexp_replace(regexp_replace(Rating,"\"",""),">","") from BookRating;

select count(*) from bookrating
select count(*) from book_ratingi
// Both should yield same count of 1149780
 
INSERT OVERWRITE TABLE BookData
SELECT BookData.*
FROM BookData WHERE YearOfPublication > 0;
/* There are 4619 book entries with 0 as year */

QUESTION : Find out the frequency of books published each year.
Answer:  Should result in 114 unique records and 115 if YearOfPublication =0 is included
	SELECT YearOfPublication, count(DISTINCT BookTitle) as Count_of_Pubs 
	from BookData group by YearOfPublication sort by Count_of_Pubs DESC;

QUESTION : Find out in which year maximum number of books were published
Answer : 17,628 - This job runs 2 jobs in hadoop (first to count of all and then limits to 1)
	SELECT YearOfPublication, COUNT(BookTitle) as Count_of_Pubs 
		FROM BookData GROUP BY YearOfPublication sort by Count_of_Pubs DESC limit 1;


Question : JOINING BOTH THE TABLES - BOOKS AND BOOK_RATING_INT
	CREATE TABLE book_join (ISBN STRING, YearOfPublication INT, BookTitle STRING, Rating INT) 
		ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";
	
	INSERT OVERWRITE TABLE book_join
		select a.ISBN, regexp_replace(regexp_replace(a.YearOfPublication,"\"",""),">",""), a.BookTitle, b.Rating
			from BookData a join book_ratingi b on a.ISBN = b.ISBN;

Stinger Initiative Hive 0.13, no use of JOIN
select a.ISBN, regexp_replace(regexp_replace(a.YearOfPublication,"\"",""),">",""), a.BookTitle, b.Rating
	from BookData a ,book_ratingi b on a.ISBN = b.ISBN;


QUESTION : Find out how many book were published based on ranking in the year 2002.
	Create Table Rating_2002 (Rating INT, Count_of_Books INT) 
		ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";
	
	INSERT OVERWRITE TABLE Rating_2002
		Select Rating, count(BookTitle) from book_join 
			where YearOfPublication = 2002 group by Rating order by Rating asc;
			
Select distinct rating from bookrating;


Books User data ---------------------------------------------------------------------------
 CREATE TABLE IF NOT EXISTS UserData 
  (UserId STRING, 
  Location STRING, 
  Age INT)
  ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '\59'
  STORED AS TEXTFILE;
  
 LOAD DATA INPATH '/user/hive/Data/BX-UsersC.csv'  
 OVERWRITE INTO TABLE UserData;

CREATE TABLE IF NOT EXISTS users 
(user_id INT, location STRING, age INT) 
COMMENT 'Book Crossing users cleaned' 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\073' STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH 'BX-Users-prepro.txt' OVERWRITE INTO TABLE users; 

use bookcrossing;
alter table users rename to bookcrossing.users;

Or 

CREATE TABLE bookcrossing.users
AS SELECT * FROM users ;
DROP TABLE users ;

Creating View……
	CREATE TABLE tmp_users AS SELECT user_id, location, age FROM users WHERE age>18 and age <34;
	INSERT INTO TABLE tmp_users SELECT user_id, location, age FROM users WHERE age>33 and age <51;
	CREATE VIEW tmp_users_view AS SELECT user_id, location, age FROM users WHERE age>18 and age <34;


Order by , GROUP By, CLUSTER by ,SORT by, DISTRIBUTE by, UNION ALL, IN, NOT IN, EXISTS , NOT EXISTS, DISTINCT
describe formatted UserData;
select DISTINCT rating from bookrating;  -- 11 records
SELECT user_id, location, age FROM UserData WHERE age>18 and age <34 limit 10; 
SELECT count(*) FROM UserData WHERE age>18 and age <34 ; 
SELECT age, count(*) FROM UserData GROUP BY age; 
SELECT age, count(*) as c FROM UserData GROUP BY age ORDER BY c DESC;
EXPLAIN SELECT userid, location, age FROM UserData WHERE age>18 and age <34 limit 10;
Select * from tmp_users ORDER by age limit 10; -- 15s
Select * from tmp_users CLUSTER by age limit 10; -- 25-60s / 2 reducers
Select * from tmp_users ORDER by age limit 10; -- 25-60s / 2 reducers
Select * from tmp_users DISTRIBUTE by age limit 10; -- 50s DOES NOT SORT, distributes different age group data to reducers

Select count(*) from (
	SELECT user_id, location, age FROM users WHERE age>18 and age <34
		UNION ALL
	SELECT user_id, location, age FROM users WHERE age>33 and age <51
) unionresult

Select * from userdata where userid in ( select userid from bookrating ) LIMIT 10;
Select * from userdata where userid not in ( select userid from bookrating ) LIMIT 10; --userid = 1 is one of them
Also check this link on above topics -> http://www.datascience-labs.com/hive/hiveql-joins/

HIVE Storage formats                                                                       : 
	ORC file - Optimized row columnar - Execute plan uses vectorization to retrive data ( get 1000 at a time usually 1 record) 
		CREATE TABLE IF NOT EXISTS users_orc (user_id INT, location STRING, age INT) COMMENT 'Book Crossing users table ORC format' STORED AS ORC;
		INSERT INTO TABLE users_orc SELECT * FROM users;
	
	RC file
		CREATE TABLE TwitterExampleRCtable(
        tweetId INT, username BIGINT,
        txt STRING, CreatedAt STRING,
        profileLocation STRING COMMENT 'Location of user',
        favc INT,retweet STRING,retcount INT,followerscount INT)
    COMMENT 'This is the Twitter streaming data'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t'
    STORED AS RCFILE;


HIVE Functions                                                                                   : 
	SHOW FUNCTIONS;
	describe function extended parse_url;
	select isbn, parse_url(image_s, 'FILE') from Book limit 10; 
	Select * , RANK() OVER (order by rating desc) as rank from bookrating limit 10;
	Select * , RANK() OVER (partition by userid) as rank from bookrating limit 10;
	Select * , RANK() OVER (partition by userid order by rating desc) as rank from bookrating limit 10;
	Select * from (Select * , RANK() OVER (partition by userid order by rating desc) as rank from bookrating) as ranked_rating where ranked_rating.rank=1 limit 10;   - Gives the top rated books by all users in the rating table
	Select * from (Select * , RANK() OVER (partition by userid order by rating desc) as rank from bookrating) as ranked_rating where ranked_rating.rank=1 and rating =10 limit 10; 
	Select isbn, count(*) from (Select * , RANK() OVER (partition by userid order by rating desc) as rank from bookrating) as ranked_rating where ranked_rating.rank=1 and rating =10 group by isbn order by isbn limit 10; 
	

Select isbn,rating,count(*) as cnt from bookrating where rating =10 group by isbn,rating order by cnt desc limit 1;
Answer = 0385504209 with 158 rating "10" rating

Question :Books that got maximum 0 rating, maximum 1 rating, maximum 1 rating, and so on
Create table ratingcnt  as Select isbn,rating,count(*) as cnt from bookrating group by isbn,rating;
Select * from (
Select *, rank() over (partition by ratin order bt cnt desc) as rank from ratingcnt 
) rankedcnt where rankedcnt.rank=1 and rating is not null limit 20;
Answer = Output of 11 records for each distinct rating 0 to 10 and books that got that rating !


Column Aliases,CASE … WHEN … THEN Statements,Calc WHERE Clauses & LIKE and RLIKE  
COLLECTION, MAP, ARRAY & STRUCT :   
	CREATE TABLE employees (
	name STRING,
	salary FLOAT,
	subordinates ARRAY<STRING>,
	deductions MAP<STRING, FLOAT>,
	address STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>
	)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '\073'
	COLLECTION ITEMS TERMINATED BY '\072'
	MAP KEYS TERMINATED BY '\043'
	LINES TERMINATED BY '\n'
	STORED AS TEXTFILE;
	load data inpath '/user/hive/Data/employee1.txt' overwrite into table employees;
	
	Data for above scripts…………employee1.txt-----------
	John Doe;100000.0;Mary Smith:Todd Jones;Federal Taxes#.2:State Taxes#.05:Insurance#.1;1 Michigan Ave.:Chicago:IL:60600
	Mary Smith;80000.0;Bill King;Federal Taxes#.2:State Taxes#.05:Insurance#.1;100 Ontario St.:Chicago:IL:60601
	Todd Jones;70000.0;;Federal Taxes#.15:State Taxes#.03:Insurance#.1;200 Chicago Ave.:Oak Park:IL:60700
	Bill King;60000.0;;Federal Taxes#.15:State Taxes#.03:Insurance#.1;300 Obscure Dr.:Obscuria:IL:60100
	Data for above scripts…………employee1.txt-----------
	Execute all the queries here -> http://www.datascience-labs.com/hive/hiveql-data-manipulation/
	
	SELECT name, address FROM employees
	WHERE address.street RLIKE '.*(Chicago|Ontario).*';
	
	SELECT name, salary,
	CASE
	WHEN salary < 50000.0 THEN 'low'
	WHEN salary >= 50000.0 AND salary < 70000.0 THEN 'middle'
	WHEN salary >= 70000.0 AND salary < 100000.0 THEN 'high'
	ELSE 'very high'
	END AS bracket FROM employees;
	
	FROM (
	SELECT upper(name), salary, deductions["Federal Taxes"] as fed_taxes,
	round(salary * (1 - deductions["Federal Taxes"])) as salary_minus_fed_taxes
	FROM employees
	) e
	SELECT e.name, e.salary_minus_fed_taxes
	WHERE e.salary_minus_fed_taxes > 70000;
	JOHN DOE 100000.0 0.2 80000
	

Partitions, Buckets, Managed, External                                                     : 
	http://hortonworks.com/hadoop-tutorial/using-hive-data-analysis/
	1. Managed tables vs external tables
			i. Managed:
    CREATE TABLE ManagedExample(
        tweetId BIGINT, username STRING,
        txt STRING, CreatedAt STRING,
        profileLocation STRING,
        favc BIGINT,retweet STRING,retcount BIGINT,followerscount BIGINT)
    COMMENT 'This is the Twitter streaming data'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t'
    STORED AS TEXTFILE;
External:
    CREATE EXTERNAL TABLE IF NOT EXISTS ExternalExample(
        tweetId BIGINT, username STRING,
        txt STRING, CreatedAt STRING,
        profileLocation STRING,
        favc BIGINT,retweet STRING,retcount BIGINT,followerscount BIGINT)
    COMMENT 'This is the Twitter streaming data'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t'
    STORED AS TEXTFILE
    location '/user/Twitterdata.txt';
	2. PARTITIONED a Table
			CREATE TABLE PARTITIONEDExample(
tweetId INT, username BIGINT, txt STRING,favc INT,retweet STRING,retcount INT,followerscount INT) COMMENT 'This is the Twitter streaming data' PARTITIONED BY(CreatedAt STRING, profileLocation STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
			FROM twitterexampletextexample
INSERT OVERWRITE TABLE PARTITIONEDExample PARTITION (CreatedAt="26 04:50:56 UTC 2014",profileLocation="Chicago") SELECT tweetId,username,txt,favc,retweet,retcount,followerscount where profileLocation='Chicago' limit 100;
			
			i. Creates a folder in HDFS folder "PARTITIONEDExample" called CreatedAt="26 04:50:56 UTC 2014" and another subdirectory of profileLocation="Chicago"
	3. Bucketing a Table
		CREATE TABLE BucketingExample(
        tweetId INT, username BIGINT,
        txt STRING,CreatedAt STRING,favc INT,retweet STRING,retcount INT,                           followerscount INT)
    COMMENT 'This is the Twitter streaming data'
    PARTITIONED BY( profileLocation STRING)
    CLUSTERED BY(tweetId) INTO 2 BUCKETS
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t'
    STORED AS TEXTFILE;
		set hive.enforce.bucketing = true; 
    FROM twitterexampletextexample
    INSERT OVERWRITE TABLE BucketingExample PARTITION (profileLocation="Chicago")    SELECT tweetId,username,txt,CreatedAt,favc,retweet,retcount,followerscount       where profileLocation='Chicago' limit 100;
		
	
	Custom UDF Functions                                                                                   : 
		http://dev.bizo.com/2009/06/custom-udfs-and-hive.html
		Stateful UDF -> https://gist.github.com/yssharma/4368970  -- Oracle sequence for the session
		
	
	
	

http://drill.apache.org/docs/json-data-model/

https://github.com/ananddhanabal/programmingpig

Books = LOAD  '/user/hive/Data/BX-BooksC.csv' 
	USING PigStorage(';')  AS (
		isbn:chararray, 
		title:chararray, 
		author:chararray, 
		year:int, 
		publisher:chararray, 
		image_s:chararray, 
		image_m:chararray, 
		image_l:chararray);
Ratings = LOAD  '/user/hive/Data/BX-BookRatingC.csv'
	USING PigStorage(';')  AS (
		userid:int, 
		isbn:chararray, 
		ratings:int);
GoodRatings = FILTER Ratings BY ratings > 3;
J = JOIN Books BY isbn, GoodRatings by isbn;
JA = GROUP J BY author;
JB = FOREACH JA GENERATE COUNT(J), group;
OA = LIMIT JB 100;
DUMP OA; 



--------------------------------------------Book Data--------Hive/Pig End--------------------------------------------
